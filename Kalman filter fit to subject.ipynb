{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "import os\n",
    "import os.path\n",
    "import statistics as stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SMEP(theta):\n",
    "    v1 = 0.5  # Prior belief mean reward value at trial 1, chance level\n",
    "    sig1 = 0.5  # Prior belief variance at trial 1\n",
    "    sigO = 0.25  # Observation variance\n",
    "    sigD = 0.1  # Diffusion variance\n",
    "    decay = 0.99  # Decay parameter\n",
    "    decay_center = 0.5  # Decay center\n",
    "\n",
    "    datapath = '/Users/sijinchen/Desktop/Cathy Caroline paper/summary data for RL/'\n",
    "    df = pd.read_csv(datapath + '//' + str(number) +'.csv')\n",
    "    choices= df['choice'].tolist()\n",
    "    rewards = df['outcome'].tolist()\n",
    "\n",
    "    beta = theta[0]  # Exploration bonus parameter\n",
    "    phi = theta[1]  # Uncertainty influence parameter\n",
    "    persev = theta[2]  # # Perseveration bonus parameter\n",
    "\n",
    "    # Initial values for reward belief and uncertainty\n",
    "    v = np.full(3, v1)  # Initial beliefs about mean rewards for each option\n",
    "    sig = np.full(3, sig1)  # Initial uncertainties for each option\n",
    "\n",
    "    # Store probability given model, use to calculate model likelihood\n",
    "    probList = []\n",
    "\n",
    "\n",
    "    for t in range(len(rewards)):\n",
    "\n",
    "        # Exploration and Perseveration Bonuses\n",
    "        eb = phi * sig  # Exploration bonus based on current uncertainty\n",
    "        pb = np.zeros(3)  # Perseveration bonus initialized to 0 for all options\n",
    "\n",
    "        if t == 0:  ### first trial\n",
    "             pChoice = 1/3   ### first choice is random, p (choice) is 0.33\n",
    "             probList.append(pChoice)\n",
    "\n",
    "        else:\n",
    "            pb[choices[t-1]] = persev  # Apply perseveration bonus to the previous choice, only tracking one trial back\n",
    "\n",
    "            # Update beliefs using a simplified Kalman filter approach\n",
    "            pe = rewards[t-1] - v[choices[t-1]]  # Prediction error\n",
    "\n",
    "            Kgain = sig[choices[t-1]]**2 / (sig[choices[t-1]]**2 + sigO**2)  # Kalman gain\n",
    "            \n",
    "            v[choices[t-1]] += Kgain * pe  # Update the mean belief\n",
    "            \n",
    "            sig[choices[t-1]] = np.sqrt((1 - Kgain) * sig[choices[t-1]]**2)  # Update the uncertainty\n",
    "\n",
    "            # Apply decay to beliefs and uncertainties\n",
    "            v = decay * v + (1 - decay) * decay_center\n",
    "            sig = np.sqrt(decay**2 * sig**2 + sigD**2)\n",
    "\n",
    "            # Action Selection based on the current Utility (value + bonuses)\n",
    "            utilities = v + eb + pb  # Calculate utilities for all options\n",
    "            \n",
    "            probabilities = np.exp(utilities*beta) / np.sum(np.exp(utilities*beta))  # Softmax transformation to probabilities\n",
    "            \n",
    "            pChoice = probabilities[choices[t]]\n",
    "            probList.append(pChoice)\n",
    "\n",
    "    ##log likelihood\n",
    "    logLike = (np.sum(np.log(probList)))*(-1)\n",
    "\n",
    "    return logLike\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_SMEP():\n",
    "\n",
    "    nll = []\n",
    "    betaList = []\n",
    "    phiList = []\n",
    "    persevList = []\n",
    "    IDlist = []\n",
    "\n",
    "\n",
    "    global number\n",
    "    for number in range (1,410):\n",
    "        IDlist.append (number)\n",
    "        print (\"*************************************************\")\n",
    "        print (\"This is ID number \" + str(number))\n",
    "\n",
    "\n",
    "        replication = 10\n",
    "        bnds = ((0.1,10),(-5,5), (0,1),)\n",
    "\n",
    "        for h in range (replication):\n",
    "            # print (\"*************************************************\")\n",
    "            # print (\"This is replication number \" + str(h))\n",
    "            beta = np.random.uniform(0.5,2)  ## starting from moderate weight\n",
    "            phi = np.random.uniform(0.01,0.2)\n",
    "            persev = np.random.uniform(0.01,0.2) ## starting from low perseveration\n",
    "\n",
    "            startParams = [beta, phi, persev]\n",
    "            theta_optim = minimize(SMEP, startParams, method = \"L-BFGS-B\", bounds = bnds, tol = 0.0005)\n",
    "\n",
    "            if h == 0:\n",
    "                minimizeLog = theta_optim.fun\n",
    "                optimX = theta_optim.x\n",
    "            else:\n",
    "                print (theta_optim.x)\n",
    "                print (theta_optim.fun)\n",
    "                if theta_optim.fun < minimizeLog:\n",
    "                    minimizeLog = theta_optim.fun\n",
    "                    optimX = theta_optim.x\n",
    "\n",
    "        nll.append(minimizeLog)\n",
    "        theta =  (optimX.tolist())\n",
    "        betaList.append(theta[0])\n",
    "        phiList.append(theta[1])\n",
    "        persevList.append(theta[2])\n",
    "     \n",
    "\n",
    "        dataDict = { 'numeric ID': IDlist,'random exploration': betaList, 'directed exploration': phiList, 'perseveration': persevList, 'model likelihood': nll}\n",
    "        data_df = pd.DataFrame(dataDict)\n",
    "        data_df.to_csv('bayesian learner neg phi.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_SMEP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bayesian_RL(theta):\n",
    "    v1 = 0.5  # Prior belief mean reward value at trial 1\n",
    "    sig1 = 0.5  # Prior belief variance at trial 1\n",
    "    sigO = 0.25  # Observation variance\n",
    "    # sigD = 0.1  # Diffusion variance\n",
    "    decay = 0.99  # Decay parameter\n",
    "    decay_center = 0.5  # Decay center\n",
    "\n",
    "    datapath = '/Users/sijinchen/Desktop/Cathy Caroline paper/summary data for RL/'\n",
    "    df = pd.read_csv(datapath + '//' + str(number) +'.csv')\n",
    "    choices= df['choice'].tolist()\n",
    "    rewards = df['outcome'].tolist()\n",
    "\n",
    "    beta = theta[0]  # Exploration bonus parameter\n",
    "    phi = theta[1]  # Uncertainty influence parameter\n",
    "    persev = theta[2]  # # Perseveration bonus parameter\n",
    "    sigD = theta[3]  ### diffusion variance as a free parameter\n",
    "\n",
    "    # Initial values for reward belief and uncertainty\n",
    "    v = np.full(3, v1)  # Initial beliefs about mean rewards for each option\n",
    "    sig = np.full(3, sig1)  # Initial uncertainties for each option\n",
    "\n",
    "    # Store probability given model, use to calculate model likelihood\n",
    "    probList = []\n",
    "\n",
    "\n",
    "    for t in range(len(rewards)):\n",
    "\n",
    "        # Exploration and Perseveration Bonuses\n",
    "        eb = phi * sig  # Exploration bonus based on current uncertainty\n",
    "        pb = np.zeros(3)  # Perseveration bonus initialized to 0 for all options\n",
    "\n",
    "        if t == 0:  ### first trial\n",
    "             pChoice = 1/3   ### first choice is random, p (choice) is 0.33\n",
    "             probList.append(pChoice)\n",
    "\n",
    "        else:\n",
    "            pb[choices[t-1]] = persev  # Apply perseveration bonus to the previous choice, only tracking one trial back\n",
    "\n",
    "            # Update beliefs using a simplified Kalman filter approach\n",
    "            pe = rewards[t-1] - v[choices[t-1]]  # Prediction error\n",
    "\n",
    "            Kgain = sig[choices[t-1]]**2 / (sig[choices[t-1]]**2 + sigO**2)  # Kalman gain\n",
    "            \n",
    "            v[choices[t-1]] += Kgain * pe  # Update the mean belief\n",
    "            \n",
    "            sig[choices[t-1]] = np.sqrt((1 - Kgain) * sig[choices[t-1]]**2)  # Update the uncertainty\n",
    "\n",
    "            # Apply decay to beliefs and uncertainties\n",
    "            v = decay * v + (1 - decay) * decay_center\n",
    "            sig = np.sqrt(decay**2 * sig**2 + sigD**2)\n",
    "\n",
    "            # Action Selection based on the current Utility (value + bonuses)\n",
    "            utilities = v + eb + pb  # Calculate utilities for all options\n",
    "            \n",
    "            probabilities = np.exp(utilities*beta) / np.sum(np.exp(utilities*beta))  # Softmax transformation to probabilities\n",
    "            \n",
    "            pChoice = probabilities[choices[t]]\n",
    "            probList.append(pChoice)\n",
    "\n",
    "    ##log likelihood\n",
    "    logLike = (np.sum(np.log(probList)))*(-1)\n",
    "\n",
    "    return logLike\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_bayesian_RL():\n",
    "\n",
    "    nll = []\n",
    "    betaList = []\n",
    "    phiList = []\n",
    "    persevList = []\n",
    "    volatList = []\n",
    "    IDlist = []\n",
    "\n",
    "\n",
    "    global number\n",
    "    for number in range (401,410):\n",
    "        IDlist.append (number)\n",
    "        print (\"*************************************************\")\n",
    "        print (\"This is ID number \" + str(number))\n",
    "\n",
    "\n",
    "        replication = 30\n",
    "        bnds = ((0.1,10),(0.01,10), (0,1),(0,1))\n",
    "\n",
    "        for h in range (replication):\n",
    "            # print (\"*************************************************\")\n",
    "            # print (\"This is replication number \" + str(h))\n",
    "            beta = np.random.uniform(0.5,2)  ## starting from moderate weight\n",
    "            phi = np.random.uniform(0.01,0.2)\n",
    "            persev = np.random.uniform(0.01,0.2) ## starting from low perseveration\n",
    "            sigD = np.random.uniform(0.01,0.5)\n",
    "\n",
    "            startParams = [beta, phi, persev,sigD]\n",
    "            theta_optim = minimize(bayesian_RL, startParams, method = \"L-BFGS-B\", bounds = bnds, tol = 0.0005)\n",
    "\n",
    "            if h == 0:\n",
    "                minimizeLog = theta_optim.fun\n",
    "                optimX = theta_optim.x\n",
    "            else:\n",
    "                print (theta_optim.x)\n",
    "                print (theta_optim.fun)\n",
    "                if theta_optim.fun < minimizeLog:\n",
    "                    minimizeLog = theta_optim.fun\n",
    "                    optimX = theta_optim.x\n",
    "\n",
    "        nll.append(minimizeLog)\n",
    "        theta =  (optimX.tolist())\n",
    "        betaList.append(theta[0])\n",
    "        phiList.append(theta[1])\n",
    "        persevList.append(theta[2])\n",
    "        volatList.append(theta[3])\n",
    "     \n",
    "\n",
    "        dataDict = { 'numeric ID': IDlist,'random exploration': betaList, 'directed exploration': phiList, 'perseveration': persevList,'diffusion variance': volatList, 'model likelihood': nll}\n",
    "        data_df = pd.DataFrame(dataDict)\n",
    "        data_df.to_csv('Bayesian RL model parameters_new400.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_bayesian_RL()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bayesian_RL_sig0(theta):\n",
    "    v1 = 0.5  # Prior belief mean reward value at trial 1\n",
    "    sig1 = 0.5  # Prior belief variance at trial 1\n",
    "    # sigO = 0.25  # Observation variance\n",
    "    # sigD = 0.1  # Diffusion variance\n",
    "    decay = 0.99  # Decay parameter\n",
    "    decay_center = 0.5  # Decay center\n",
    "\n",
    "    datapath = '/Users/sijinchen/Desktop/Cathy Caroline paper/summary data for RL/'\n",
    "    df = pd.read_csv(datapath + '//' + str(number) +'.csv')\n",
    "    choices= df['choice'].tolist()\n",
    "    rewards = df['outcome'].tolist()\n",
    "\n",
    "    beta = theta[0]  # Exploration bonus parameter\n",
    "    phi = theta[1]  # Uncertainty influence parameter\n",
    "    persev = theta[2]  # # Perseveration bonus parameter\n",
    "    sigD = theta[3]  ### diffusion variance as a free parameter\n",
    "    sigO = theta[4]  ###observation variance as a free paramter\n",
    "\n",
    "    # Initial values for reward belief and uncertainty\n",
    "    v = np.full(3, v1)  # Initial beliefs about mean rewards for each option\n",
    "    sig = np.full(3, sig1)  # Initial uncertainties for each option\n",
    "\n",
    "    # Store probability given model, use to calculate model likelihood\n",
    "    probList = []\n",
    "\n",
    "\n",
    "    for t in range(len(rewards)):\n",
    "\n",
    "        # Exploration and Perseveration Bonuses\n",
    "        eb = phi * sig  # Exploration bonus based on current uncertainty\n",
    "        pb = np.zeros(3)  # Perseveration bonus initialized to 0 for all options\n",
    "\n",
    "        if t == 0:  ### first trial\n",
    "             pChoice = 1/3   ### first choice is random, p (choice) is 0.33\n",
    "             probList.append(pChoice)\n",
    "\n",
    "        else:\n",
    "            pb[choices[t-1]] = persev  # Apply perseveration bonus to the previous choice, only tracking one trial back\n",
    "\n",
    "            # Update beliefs using a simplified Kalman filter approach\n",
    "            pe = rewards[t-1] - v[choices[t-1]]  # Prediction error\n",
    "\n",
    "            Kgain = sig[choices[t-1]]**2 / (sig[choices[t-1]]**2 + sigO**2)  # Kalman gain\n",
    "            \n",
    "            v[choices[t-1]] += Kgain * pe  # Update the mean belief\n",
    "            \n",
    "            sig[choices[t-1]] = np.sqrt((1 - Kgain) * sig[choices[t-1]]**2)  # Update the uncertainty\n",
    "\n",
    "            # Apply decay to beliefs and uncertainties\n",
    "            v = decay * v + (1 - decay) * decay_center\n",
    "            sig = np.sqrt(decay**2 * sig**2 + sigD**2)\n",
    "\n",
    "            # Action Selection based on the current Utility (value + bonuses)\n",
    "            utilities = v + eb + pb  # Calculate utilities for all options\n",
    "            \n",
    "            probabilities = np.exp(utilities*beta) / np.sum(np.exp(utilities*beta))  # Softmax transformation to probabilities\n",
    "            \n",
    "            pChoice = probabilities[choices[t]]\n",
    "            probList.append(pChoice)\n",
    "\n",
    "    ##log likelihood\n",
    "    logLike = (np.sum(np.log(probList)))*(-1)\n",
    "\n",
    "    return logLike\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_bayesian_RL2():\n",
    "\n",
    "    nll = []\n",
    "    betaList = []\n",
    "    phiList = []\n",
    "    persevList = []\n",
    "    volatList = []\n",
    "    IDlist = []\n",
    "    stochList = []\n",
    "\n",
    "\n",
    "    global number\n",
    "    for number in range (1,401):\n",
    "        IDlist.append (number)\n",
    "        print (\"*************************************************\")\n",
    "        print (\"This is ID number \" + str(number))\n",
    "\n",
    "\n",
    "        replication = 30\n",
    "        bnds = ((0.1,10),(0.01,10), (0,1),(0,1),(0,1))\n",
    "\n",
    "        for h in range (replication):\n",
    "            # print (\"*************************************************\")\n",
    "            # print (\"This is replication number \" + str(h))\n",
    "            beta = np.random.uniform(0.5,2)  ## starting from moderate weight\n",
    "            phi = np.random.uniform(0.01,0.2)\n",
    "            persev = np.random.uniform(0.01,0.2) ## starting from low perseveration\n",
    "            sigD = np.random.uniform(0.01,0.5)\n",
    "            sigO = np.random.uniform(0.01,0.5)\n",
    "\n",
    "            startParams = [beta, phi, persev,sigD, sigO]\n",
    "            theta_optim = minimize(bayesian_RL, startParams, method = \"L-BFGS-B\", bounds = bnds, tol = 0.0005)\n",
    "\n",
    "            if h == 0:\n",
    "                minimizeLog = theta_optim.fun\n",
    "                optimX = theta_optim.x\n",
    "            else:\n",
    "                print (theta_optim.x)\n",
    "                print (theta_optim.fun)\n",
    "                if theta_optim.fun < minimizeLog:\n",
    "                    minimizeLog = theta_optim.fun\n",
    "                    optimX = theta_optim.x\n",
    "\n",
    "        nll.append(minimizeLog)\n",
    "        theta =  (optimX.tolist())\n",
    "        betaList.append(theta[0])\n",
    "        phiList.append(theta[1])\n",
    "        persevList.append(theta[2])\n",
    "        volatList.append(theta[3])\n",
    "        stochList.append(theta[4])\n",
    "     \n",
    "\n",
    "        dataDict = { 'numeric ID': IDlist,'random exploration': betaList, 'directed exploration': phiList, 'perseveration': persevList,'diffusion variance': volatList,'observation variance': stochList, 'model likelihood': nll}\n",
    "        data_df = pd.DataFrame(dataDict)\n",
    "        data_df.to_csv('Bayesian RL model parameters_new.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_bayesian_RL2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SMEP_nopersev(theta):\n",
    "    v1 = 0.5  # Prior belief mean reward value at trial 1\n",
    "    sig1 = 0.25  # Prior belief variance at trial 1\n",
    "    sigO = 0.25  # Observation variance\n",
    "    sigD = 0.1  # Diffusion variance\n",
    "    decay = 0.99  # Decay parameter\n",
    "    decay_center = 0.5  # Decay center\n",
    "\n",
    "    datapath = '/Users/sijinchen/Desktop/Cathy Caroline paper/summary data for RL/'\n",
    "    df = pd.read_csv(datapath + '//' + str(number) +'.csv')\n",
    "    choices= df['choice'].tolist()\n",
    "    rewards = df['outcome'].tolist()\n",
    "\n",
    "    beta = theta[0]  # Exploration bonus parameter\n",
    "    phi = theta[1]  # Uncertainty influence parameter\n",
    "\n",
    "    # Initial values for reward belief and uncertainty\n",
    "    v = np.full(3, v1)  # Initial beliefs about mean rewards for each option\n",
    "    sig = np.full(3, sig1)  # Initial uncertainties for each option\n",
    "\n",
    "    # Store probability given model, use to calculate model likelihood\n",
    "    probList = []\n",
    "\n",
    "    for t in range(len(rewards)):\n",
    "\n",
    "        # Exploration and Perseveration Bonuses\n",
    "        eb = phi * sig  # Exploration bonus based on current uncertainty\n",
    "        pb = np.zeros(3)  # Perseveration bonus initialized to 0 for all options\n",
    "\n",
    "        if t == 0:  ### first trial\n",
    "             pChoice = 1/3   ### first choice is random, p (choice) is 0.33\n",
    "             probList.append(pChoice)\n",
    "\n",
    "        else:\n",
    "            # pb[choices[t-1]] = persev  # Apply perseveration bonus to the previous choice, only tracking one trial back\n",
    "\n",
    "            # Update beliefs using a simplified Kalman filter approach\n",
    "            pe = rewards[t-1] - v[choices[t-1]]  # Prediction error\n",
    "            # print (pe)\n",
    "            Kgain = sig[choices[t-1]]**2 / (sig[choices[t-1]]**2 + sigO**2)  # Kalman gain\n",
    "            # print (Kgain)\n",
    "            v[choices[t-1]] += Kgain * pe  # Update the mean belief\n",
    "            # print (v)\n",
    "            sig[choices[t-1]] = np.sqrt((1 - Kgain) * sig[choices[t-1]]**2)  # Update the uncertainty\n",
    "\n",
    "            # Apply decay to beliefs and uncertainties\n",
    "            v = decay * v + (1 - decay) * decay_center\n",
    "            sig = np.sqrt(decay**2 * sig**2 + sigD**2)\n",
    "\n",
    "            # Action Selection based on the current Utility (value + bonuses)\n",
    "            utilities = v + eb + pb  # Calculate utilities for all options\n",
    "            # print (utilities)\n",
    "            probabilities = np.exp(utilities*beta) / np.sum(np.exp(utilities*beta))  # Softmax transformation to probabilities\n",
    "            # print (probabilities)\n",
    "            # print (choices[t])\n",
    "            pChoice = probabilities[choices[t]]\n",
    "            probList.append(pChoice)\n",
    "\n",
    "    ##log likelihood\n",
    "    logLike = (np.sum(np.log(probList)))*(-1)\n",
    "\n",
    "    return logLike\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SMERP(theta):\n",
    "    v1 = 0.5  # Prior belief mean reward value at trial 1\n",
    "    sig1 = 0.5  # Prior belief variance at trial 1\n",
    "    sigO = 0.25  # Observation variance\n",
    "    sigD = 0.1  # Diffusion variance\n",
    "    decay = 0.99  # Decay parameter\n",
    "    decay_center = 0.5  # Decay center\n",
    "\n",
    "    datapath = '/Users/sijinchen/Desktop/Cathy Caroline paper/summary data for RL/'\n",
    "    df = pd.read_csv(datapath + '//' + str(number) +'.csv')\n",
    "    choices= df['choice'].tolist()\n",
    "    rewards = df['outcome'].tolist()\n",
    "\n",
    "    beta = theta[0]  # Exploration bonus parameter\n",
    "    phi = theta[1]  # Uncertainty influence parameter\n",
    "    persev = theta[2]  # # Perseveration bonus parameter\n",
    "    gamma = theta[3]\n",
    "\n",
    "    # Initial values for reward belief and uncertainty\n",
    "    v = np.full(3, v1)  # Initial beliefs about mean rewards for each option\n",
    "    sig = np.full(3, sig1)  # Initial uncertainties for each option\n",
    "\n",
    "    # Store probability given model, use to calculate model likelihood\n",
    "    probList = []\n",
    "\n",
    "    for t in range(len(rewards)):\n",
    "\n",
    "        # Exploration and Perseveration Bonuses\n",
    "        eb = phi * sig  # Exploration bonus based on current uncertainty\n",
    "        pb = np.zeros(3)  # Perseveration bonus initialized to 0 for all options\n",
    "\n",
    "        if t == 0:  ### first trial\n",
    "             pChoice = 1/3   ### first choice is random, p (choice) is 0.33\n",
    "             probList.append(pChoice)\n",
    "\n",
    "        else:\n",
    "            pb[choices[t-1]] = persev  # Apply perseveration bonus to the previous choice, only tracking one trial back\n",
    "\n",
    "            # Update beliefs using a simplified Kalman filter approach\n",
    "            pe = rewards[t-1] - v[choices[t-1]]  # Prediction error\n",
    "            # print (pe)\n",
    "            Kgain = sig[choices[t-1]]**2 / (sig[choices[t-1]]**2 + sigO**2)  # Kalman gain\n",
    "            # print (Kgain)\n",
    "            v[choices[t-1]] += Kgain * pe  # Update the mean belief\n",
    "            # print (v)\n",
    "            sig[choices[t-1]] = np.sqrt((1 - Kgain) * sig[choices[t-1]]**2)  # Update the uncertainty\n",
    "\n",
    "            # Apply decay to beliefs and uncertainties\n",
    "            v = decay * v + (1 - decay) * decay_center\n",
    "            sig = np.sqrt(decay**2 * sig**2 + sigD**2)\n",
    "\n",
    "            # Calculate the total uncertainty\n",
    "            total_uncertainty = np.sum(sig)\n",
    "\n",
    "            # Action Selection based on the current Utility (value + bonuses)\n",
    "            utilities = v + eb + pb  # Calculate utilities for all options\n",
    "\n",
    "            # Include the new term for total uncertainty-based random exploration\n",
    "            utilities += (gamma * v) / total_uncertainty\n",
    "\n",
    "            probabilities = np.exp(utilities*beta) / np.sum(np.exp(utilities*beta))  # Softmax transformation to probabilities\n",
    "            # print (probabilities)\n",
    "            # print (choices[t])\n",
    "            pChoice = probabilities[choices[t]]\n",
    "            probList.append(pChoice)\n",
    "\n",
    "    ##log likelihood\n",
    "    logLike = (np.sum(np.log(probList)))*(-1)\n",
    "\n",
    "    return logLike\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SMEP_with_L2(params):\n",
    "    beta, phi, persev = params\n",
    "    original_SMEP_value = SMEP(params)  # Your original SMEP function calculation\n",
    "    \n",
    "    # L2 Regularization Term\n",
    "    lambda_reg = 0.001  # Regularization strength, adjust based on your needs\n",
    "    l2_penalty = lambda_reg * (beta**2 + phi**2 + persev**2)\n",
    "    \n",
    "    # Modified Objective Function with L2 Regularization\n",
    "    return original_SMEP_value + l2_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SMERP_with_L2(params):\n",
    "    beta, phi, persev, gamma = params\n",
    "    original_SMEP_value = SMERP(params)  # Your original SMEP function calculation\n",
    "    \n",
    "    # L2 Regularization Term\n",
    "    lambda_reg = 0.5  # Regularization strength, adjust based on your needs\n",
    "    l2_penalty = lambda_reg * (beta**2 + phi**2 + persev**2 + gamma**2)\n",
    "    \n",
    "    # Modified Objective Function with L2 Regularization\n",
    "    return original_SMEP_value + l2_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SMEP_with_attractor(theta):\n",
    "    v1 = 0.5  # Prior belief mean reward value at trial 1\n",
    "    sig1 = 0.5  # Prior belief variance at trial 1\n",
    "    sigO = 0.25  # Observation variance\n",
    "    sigD = 0.1  # Diffusion variance\n",
    "    decay = 0.99  # Decay parameter\n",
    "    decay_center = 0.5  # Decay center\n",
    "\n",
    "    datapath = '/Users/sijinchen/Desktop/Cathy Caroline paper/summary data for RL/'\n",
    "    df = pd.read_csv(datapath + '//' + str(number) +'.csv')\n",
    "    choices= df['choice'].tolist()\n",
    "    rewards = df['outcome'].tolist()\n",
    "\n",
    "    beta = theta[0]  # Exploration bonus parameter\n",
    "    phi = theta[1]  # Uncertainty influence parameter\n",
    "    persev = theta[2]  # # Perseveration bonus parameter\n",
    "    attractor = theta[3] ## attractor strength\n",
    "\n",
    "    # Initial values for reward belief and uncertainty\n",
    "    v = np.full(3, v1)  # Initial beliefs about mean rewards for each option\n",
    "    sig = np.full(3, sig1)  # Initial uncertainties for each option\n",
    "\n",
    "    # Store probability given model, use to calculate model likelihood\n",
    "    probList = []\n",
    "\n",
    "    for t in range(len(rewards)):\n",
    "\n",
    "        # Exploration and Perseveration Bonuses\n",
    "        eb = phi * sig  # Exploration bonus based on current uncertainty\n",
    "        pb = np.zeros(3)  # Perseveration bonus initialized to 0 for all options\n",
    "\n",
    "        if t == 0:  ### first trial\n",
    "             pChoice = 1/3   ### first choice is random, p (choice) is 0.33\n",
    "             probList.append(pChoice)\n",
    "\n",
    "        else:\n",
    "            pb[choices[t-1]] = persev  # Apply perseveration bonus to the previous choice, only tracking one trial back\n",
    "\n",
    "            # Update beliefs using a simplified Kalman filter approach\n",
    "            pe = rewards[t-1] - v[choices[t-1]]  # Prediction error\n",
    "            # print (pe)\n",
    "            Kgain = sig[choices[t-1]]**2 / (sig[choices[t-1]]**2 + sigO**2)  # Kalman gain\n",
    "            # print (Kgain)\n",
    "            v[choices[t-1]] += Kgain * pe  # Update the mean belief\n",
    "            # print (v)\n",
    "            sig[choices[t-1]] = np.sqrt((1 - Kgain) * sig[choices[t-1]]**2)  # Update the uncertainty\n",
    "\n",
    "            # Apply decay to beliefs and uncertainties\n",
    "            v = decay * v + (1 - decay) * decay_center\n",
    "            sig = np.sqrt(decay**2 * sig**2 + sigD**2)\n",
    "\n",
    "            # Calculate attractor bonus for each option based on its uncertainty\n",
    "            attractor_bonus = attractor / (sig + 1e-6)\n",
    "\n",
    "            # Action Selection based on the current Utility (value + bonuses)\n",
    "            utilities = v + eb + pb + attractor_bonus # Calculate utilities for all options\n",
    "            # print (utilities)\n",
    "            probabilities = np.exp(utilities*beta) / np.sum(np.exp(utilities*beta))  # Softmax transformation to probabilities\n",
    "            # print (probabilities)\n",
    "            # print (choices[t])\n",
    "            pChoice = probabilities[choices[t]]\n",
    "            probList.append(pChoice)\n",
    "\n",
    "    ##log likelihood\n",
    "    logLike = (np.sum(np.log(probList)))*(-1)\n",
    "\n",
    "    return logLike\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_SMEP_with_attractors():\n",
    "\n",
    "    nll = []\n",
    "    betaList = []\n",
    "    phiList = []\n",
    "    persevList = []\n",
    "    attractList = []\n",
    "    IDlist = []\n",
    "\n",
    "\n",
    "    global number\n",
    "    for number in range (1,337):\n",
    "        IDlist.append (number)\n",
    "        print (\"*************************************************\")\n",
    "        print (\"This is ID number \" + str(number))\n",
    "\n",
    "\n",
    "        replication = 30\n",
    "        bnds = ((0.1,10),(0.1,5), (0,1), (0,1),)\n",
    "\n",
    "        for h in range (replication):\n",
    "            # print (\"*************************************************\")\n",
    "            # print (\"This is replication number \" + str(h))\n",
    "            beta = np.random.uniform(0.5,2)  ## starting from moderate weight\n",
    "            phi = np.random.uniform(0,1)\n",
    "            persev = np.random.uniform(0,0.2) ## starting from low perseveration\n",
    "            attractor = np.random.uniform(0.4,0.6)\n",
    "\n",
    "            startParams = [beta, phi, persev, attractor]\n",
    "            theta_optim = minimize(SMEP_with_attractor, startParams, method = \"L-BFGS-B\", bounds = bnds, tol = 0.0005)\n",
    "\n",
    "            if h == 0:\n",
    "                minimizeLog = theta_optim.fun\n",
    "                optimX = theta_optim.x\n",
    "            else:\n",
    "                print (theta_optim.x)\n",
    "                print (theta_optim.fun)\n",
    "                if theta_optim.fun < minimizeLog:\n",
    "                    minimizeLog = theta_optim.fun\n",
    "                    optimX = theta_optim.x\n",
    "\n",
    "        nll.append(minimizeLog)\n",
    "        theta =  (optimX.tolist())\n",
    "        betaList.append(theta[0])\n",
    "        phiList.append(theta[1])\n",
    "        persevList.append(theta[2])\n",
    "        attractList.append(theta[3])\n",
    "     \n",
    "\n",
    "        # dataDict = { 'numeric ID': IDlist,'random exploration': betaList, 'directed exploration': phiList, 'perseveration': persevList, 'model likelihood': nll}\n",
    "        # data_df = pd.DataFrame(dataDict)\n",
    "        # data_df.to_csv('SMEP model parameters_negPhi_negRho2.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_SMEP_with_attractors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_SMEP_nopersev():\n",
    "\n",
    "    nll = []\n",
    "    betaList = []\n",
    "    phiList = []\n",
    "    IDlist = []\n",
    "\n",
    "\n",
    "    global number\n",
    "    for number in range (1,337):\n",
    "        IDlist.append (number)\n",
    "        print (\"*************************************************\")\n",
    "        print (\"This is ID number \" + str(number))\n",
    "\n",
    "\n",
    "        replication = 30\n",
    "        bnds = ((0.1,10),(0.01,10))\n",
    "\n",
    "        for h in range (replication):\n",
    "            # print (\"*************************************************\")\n",
    "            # print (\"This is replication number \" + str(h))\n",
    "            beta = np.random.uniform(0.5,2)  ## starting from moderate weight\n",
    "            phi = np.random.uniform(0.5,2)\n",
    "\n",
    "            startParams = [beta, phi]\n",
    "            theta_optim = minimize(SMEP_nopersev, startParams, method = \"TNC\", bounds = bnds, tol = 0.0005)\n",
    "\n",
    "            if h == 0:\n",
    "                minimizeLog = theta_optim.fun\n",
    "                optimX = theta_optim.x\n",
    "            else:\n",
    "                print (theta_optim.x)\n",
    "                print (theta_optim.fun)\n",
    "                if theta_optim.fun < minimizeLog:\n",
    "                    minimizeLog = theta_optim.fun\n",
    "                    optimX = theta_optim.x\n",
    "\n",
    "        nll.append(minimizeLog)\n",
    "        theta =  (optimX.tolist())\n",
    "        betaList.append(theta[0])\n",
    "        phiList.append(theta[1])\n",
    "     \n",
    "\n",
    "        # dataDict = { 'numeric ID': IDlist,'random exploration': betaList, 'directed exploration': phiList, 'model likelihood': nll}\n",
    "        # data_df = pd.DataFrame(dataDict)\n",
    "        # data_df.to_csv('SMEP model parameters no perseveration.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_SMERP():\n",
    "\n",
    "    nll = []\n",
    "    betaList = []\n",
    "    phiList = []\n",
    "    persevList = []\n",
    "    gammaList = []\n",
    "    IDlist = []\n",
    "\n",
    "\n",
    "    global number\n",
    "    for number in range (1,337):\n",
    "        IDlist.append (number)\n",
    "        print (\"*************************************************\")\n",
    "        print (\"This is ID number \" + str(number))\n",
    "\n",
    "\n",
    "        replication = 30\n",
    "        bnds = ((0.1,10),(0.01,10), (0,1),(0,1))\n",
    "\n",
    "        for h in range (replication):\n",
    "            # print (\"*************************************************\")\n",
    "            # print (\"This is replication number \" + str(h))\n",
    "            beta = np.random.uniform(0.5,2)  ## starting from moderate weight\n",
    "            phi = np.random.uniform(0.5,2)\n",
    "            persev = np.random.uniform(0,0.2) ## starting from low perseveration\n",
    "            gamma = np.random.uniform(0.1,0.3)\n",
    "\n",
    "            startParams = [beta, phi, persev,gamma]\n",
    "            theta_optim = minimize(SMERP_with_L2, startParams, method = \"L-BFGS-B\", bounds = bnds, tol = 0.0005)\n",
    "\n",
    "            if h == 0:\n",
    "                minimizeLog = theta_optim.fun\n",
    "                optimX = theta_optim.x\n",
    "            else:\n",
    "                print (theta_optim.x)\n",
    "                print (theta_optim.fun)\n",
    "                if theta_optim.fun < minimizeLog:\n",
    "                    minimizeLog = theta_optim.fun\n",
    "                    optimX = theta_optim.x\n",
    "\n",
    "        nll.append(minimizeLog)\n",
    "        theta =  (optimX.tolist())\n",
    "        betaList.append(theta[0])\n",
    "        phiList.append(theta[1])\n",
    "        persevList.append(theta[2])\n",
    "        gammaList.append(theta[3])\n",
    "     \n",
    "\n",
    "        # dataDict = { 'numeric ID': IDlist,'random exploration': betaList, 'directed exploration': phiList, 'perseveration': persevList, 'model likelihood': nll}\n",
    "        # data_df = pd.DataFrame(dataDict)\n",
    "        # data_df.to_csv('SMEP model parameters with L2.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run each model here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_SMERP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_SMEP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_SMEP_nopersev()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### this version of the classify_trials function classifies trials into exploitation, random exploration, and directed exploration trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_trials(choices, rewards, phi, persev):\n",
    "    n_trials = len(rewards)\n",
    "    n_options = len(np.unique(choices))\n",
    "\n",
    "    # Initialize beliefs about mean rewards and uncertainties for each option\n",
    "    v = np.full(n_options, 0.5)  # Assuming binary rewards, 0.5 is a neutral initial belief\n",
    "    sig = np.full(n_options, np.sqrt(0.5 * (1 - 0.5)))  # Initial uncertainty based on Bernoulli variance\n",
    "    sig0 = 0.25\n",
    "\n",
    "    # label list\n",
    "    # exploit is 1, random  is 2, directed is 3\n",
    "    labelList = [2]  # first trial is random exploration\n",
    "\n",
    "    for t in range(1, n_trials):  # Start from 2nd trial as 1st choice is assumed random\n",
    "        eb = phi * sig  # Exploration bonus\n",
    "        utilities = v + eb\n",
    "\n",
    "        # Add perseveration bonus if applicable\n",
    "        if t > 1:\n",
    "            utilities[choices[t-1]] += persev\n",
    "\n",
    "        # Classify trials\n",
    "        chosen_option = choices[t]\n",
    "        best_option = np.argmax(v)\n",
    "\n",
    "        if chosen_option == best_option:  ## exploitation trials\n",
    "            labelList.append(1)\n",
    "        else:\n",
    "            most_uncertain_option = np.argmax(eb)\n",
    "            if chosen_option == most_uncertain_option:  ## directed exploration\n",
    "                labelList.append (3)\n",
    "            else:                           ## random exploration\n",
    "                labelList.append (2)\n",
    "\n",
    "        # Update beliefs for chosen option using a simplified model\n",
    "        pe = rewards[t] - v[chosen_option]\n",
    "        Kgain = sig[chosen_option]**2 / (sig[chosen_option]**2 + sig0**2)  # Assuming sigO=0.25 for binary rewards\n",
    "        v[chosen_option] += Kgain * pe  # Update the mean belief\n",
    "        sig[chosen_option] = np.sqrt((1 - Kgain) * sig[chosen_option]**2)  # Update the uncertainty\n",
    "\n",
    "    return labelList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trial_by_trial_variables(choices, rewards, phi, persev, ID, session):\n",
    "    n_trials = len(rewards)\n",
    "    n_options = len(np.unique(choices))\n",
    "\n",
    "    # Initialize beliefs about mean rewards and uncertainties for each option\n",
    "    v = np.full(n_options, 0.5)  # Assuming binary rewards, 0.5 is a neutral initial belief\n",
    "    sig = np.full(n_options, np.sqrt(0.5 * (1 - 0.5)))  # Initial uncertainty based on Bernoulli variance\n",
    "    sig0 = 0.25\n",
    "\n",
    "    \n",
    "    # store RPE and uncertainty bonus of chosen choice\n",
    "    PElist = []\n",
    "    kgainList = [] \n",
    "    uncertainty_relative = []\n",
    "    uncertainty_total = []\n",
    "    value_relative = []\n",
    "    utilitiesList = []\n",
    "    ebList = []\n",
    "\n",
    "    \n",
    "\n",
    "    for t in range(n_trials):  # Start from 2nd trial as 1st choice is assumed random\n",
    "        \n",
    "        chosen_option = choices[t]\n",
    "\n",
    "        # Update beliefs for chosen option using a simplified model\n",
    "        pe = rewards[t] - v[chosen_option]\n",
    "        PElist.append(pe)\n",
    "\n",
    "        Kgain = sig[chosen_option]**2 / (sig[chosen_option]**2 + sig0**2)  # Assuming sigO=0.25 for binary rewards\n",
    "        kgainList.append(Kgain)\n",
    "        \n",
    "        v[chosen_option] += Kgain * pe  # Update the mean belief\n",
    "        unchosenValue = np.delete(v, chosen_option)\n",
    "        value_relative.append(v[chosen_option]-np.mean(unchosenValue))\n",
    "        \n",
    "        sig[chosen_option] = np.sqrt((1 - Kgain) * sig[chosen_option]**2)  # Update the uncertainty\n",
    "        unchosenUncertainty =  np.delete(sig, chosen_option)\n",
    "        uncertainty_relative.append(sig[chosen_option] - np.mean(unchosenUncertainty))\n",
    "        uncertainty_total.append(np.sum(sig))\n",
    "\n",
    "        eb = phi * sig  # Exploration bonus\n",
    "        utilities = v + eb\n",
    "        ebList.append(eb[chosen_option])\n",
    "\n",
    "        # Add perseveration bonus if applicable\n",
    "        if t > 1:\n",
    "            utilities[chosen_option] += persev\n",
    "\n",
    "        utilitiesList.append(utilities[chosen_option])\n",
    "\n",
    "    trialNum = np.linspace(1, len(PElist),len(PElist))\n",
    "\n",
    "\n",
    "    trialDF = pd.DataFrame({'ID': [ID]*len(PElist), 'session': [session]*len(PElist), 'trial number': trialNum,'prediction error': PElist, 'Kalman gain': kgainList, 'relative uncertainty': uncertainty_relative, 'total uncertainty': uncertainty_total, 'relative value diff': value_relative, 'utilities': utilitiesList, 'exploration bonus':ebList})\n",
    "        \n",
    "    ## drop the first row\n",
    "    trialDF = trialDF.iloc[1:, :]\n",
    "            \n",
    "\n",
    "    return trialDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trial_by_trial_variables2(choices, rewards, phi, persev, ID, session):\n",
    "    n_trials = len(rewards)\n",
    "    n_options = len(np.unique(choices))\n",
    "\n",
    "    # Initialize beliefs about mean rewards and uncertainties for each option\n",
    "    v = np.full(n_options, 0.5)  # Assuming binary rewards, 0.5 is a neutral initial belief\n",
    "    sig = np.full(n_options, np.sqrt(0.5 * (1 - 0.5)))  # Initial uncertainty based on Bernoulli variance\n",
    "    sig0 = 0.25\n",
    "    sig = np.full(3, 0.5)  # Initial uncertainty based on Bernoulli variance\n",
    "    sig0 = 0.25\n",
    "    decayV = 0.98  # Decay parameter for value\n",
    "    decayE = 0.98\n",
    "    decay_center = 0.5  # Decay center\n",
    "    sigD = 0.1  # Diffusion variance\n",
    "\n",
    "    \n",
    "    # store RPE and uncertainty bonus of chosen choice\n",
    "    PElist = []\n",
    "    kgainList = [] \n",
    "    uncertaintyList = []\n",
    "    uncertainty_total = []\n",
    "    valueList = []\n",
    "    utilitiesList = []\n",
    "\n",
    "    \n",
    "\n",
    "    for t in range(n_trials):  # Start from 2nd trial as 1st choice is assumed random\n",
    "        \n",
    "        chosen_option = choices[t]\n",
    "\n",
    "        # Update beliefs for chosen option using a simplified model\n",
    "        pe = rewards[t] - v[chosen_option]\n",
    "        PElist.append(pe)\n",
    "\n",
    "        Kgain = sig[chosen_option]**2 / (sig[chosen_option]**2 + sig0**2)  # Assuming sigO=0.25 for binary rewards\n",
    "        kgainList.append(Kgain)\n",
    "        \n",
    "        v[chosen_option] += Kgain * pe  # Update the mean belief\n",
    "        sig[chosen_option] = np.sqrt((1 - Kgain) * sig[chosen_option]**2)  # Update the uncertainty\n",
    "\n",
    "        eb = phi * sig  # Exploration bonus\n",
    "   \n",
    "        utilities = v + eb\n",
    "\n",
    "        # Add perseveration bonus if applicable\n",
    "        if t > 1:\n",
    "            utilities[chosen_option] += persev\n",
    "\n",
    "        utilitiesList.append(utilities)\n",
    "\n",
    "        # Apply decay to beliefs and uncertainties\n",
    "        v = decayV * v + (1 - decayV) * decay_center\n",
    "        sig = np.sqrt(decayE**2 * sig**2 + sigD**2)\n",
    "\n",
    "        valueList.append(v[chosen_option])\n",
    "        uncertaintyList.append(sig[chosen_option])\n",
    "        uncertainty_total.append(np.sum(sig))\n",
    "\n",
    "    trialNum = np.linspace(1, len(PElist),len(PElist))\n",
    "\n",
    "\n",
    "    trialDF = pd.DataFrame({'ID': [ID]*len(PElist), 'session': [session]*len(PElist), 'trial number': trialNum,'prediction error': PElist, 'Kalman gain': kgainList, 'uncertainty': uncertaintyList,'total uncertain': uncertainty_total, 'value': valueList})\n",
    "        \n",
    "    ## drop the first row\n",
    "    trialDF = trialDF.iloc[1:, :]\n",
    "            \n",
    "\n",
    "    return trialDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trial_by_trial_variables3(choices, rewards, phi, persev, ID, session):\n",
    "    n_trials = len(rewards)\n",
    "    n_options = len(np.unique(choices))\n",
    "\n",
    "    # Initialize beliefs about mean rewards and uncertainties for each option\n",
    "    v = np.full(n_options, 0.5)  # Assuming binary rewards, 0.5 is a neutral initial belief\n",
    "    sig = np.full(n_options, np.sqrt(0.5 * (1 - 0.5)))  # Initial uncertainty based on Bernoulli variance\n",
    "    sig0 = 0.25\n",
    "    sig = np.full(3, 0.5)  # Initial uncertainty based on Bernoulli variance\n",
    "    sig0 = 0.25\n",
    "    decayV = 0.98  # Decay parameter for value\n",
    "    decayE = 0.98\n",
    "    decay_center = 0.5  # Decay center\n",
    "    sigD = 0.1  # Diffusion variance\n",
    "\n",
    "    \n",
    "    # store RPE and uncertainty bonus of chosen choice\n",
    "    PElist = []\n",
    "    kgainList = [] \n",
    "    uncertaintyList1 = []\n",
    "    uncertaintyList2= []\n",
    "    uncertaintyList3 = []\n",
    "    uncertainty_total = []\n",
    "    valueList = []\n",
    "    utilitiesList = []\n",
    "\n",
    "    \n",
    "\n",
    "    for t in range(n_trials):  # Start from 2nd trial as 1st choice is assumed random\n",
    "        \n",
    "        chosen_option = choices[t]\n",
    "\n",
    "        # Update beliefs for chosen option using a simplified model\n",
    "        pe = rewards[t] - v[chosen_option]\n",
    "        PElist.append(pe)\n",
    "\n",
    "        Kgain = sig[chosen_option]**2 / (sig[chosen_option]**2 + sig0**2)  # Assuming sigO=0.25 for binary rewards\n",
    "        kgainList.append(Kgain)\n",
    "        \n",
    "        v[chosen_option] += Kgain * pe  # Update the mean belief\n",
    "        sig[chosen_option] = np.sqrt((1 - Kgain) * sig[chosen_option]**2)  # Update the uncertainty\n",
    "\n",
    "        eb = phi * sig  # Exploration bonus\n",
    "   \n",
    "        utilities = v + eb\n",
    "\n",
    "        # Add perseveration bonus if applicable\n",
    "        if t > 1:\n",
    "            utilities[chosen_option] += persev\n",
    "\n",
    "        utilitiesList.append(utilities)\n",
    "\n",
    "        # Apply decay to beliefs and uncertainties\n",
    "        v = decayV * v + (1 - decayV) * decay_center\n",
    "        sig = np.sqrt(decayE**2 * sig**2 + sigD**2)\n",
    "\n",
    "        valueList.append(v[chosen_option])\n",
    "        uncertaintyList1.append(sig[0])\n",
    "        uncertaintyList2.append(sig[1])\n",
    "        uncertaintyList3.append(sig[2])\n",
    "        uncertainty_total.append(np.sum(sig))\n",
    "\n",
    "    trialNum = np.linspace(1, len(PElist),len(PElist))\n",
    "\n",
    "\n",
    "    trialDF = pd.DataFrame({'ID': [ID]*len(PElist), 'session': [session]*len(PElist), 'trial number': trialNum,'prediction error': PElist, 'Kalman gain': kgainList, 'uncertainty1': uncertaintyList1,'uncertainty2': uncertaintyList2,'uncertainty3': uncertaintyList3,'total uncertain': uncertainty_total, 'value': valueList})\n",
    "        \n",
    "    ## drop the first row\n",
    "    trialDF = trialDF.iloc[1:, :]\n",
    "            \n",
    "\n",
    "    return trialDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = '/Users/sijinchen/Desktop/Cathy Caroline paper/summary data for RL/'\n",
    "# SMEPPath = '/Users/sijinchen/Desktop/Cathy Caroline paper/SMEP label/'   ### new folder to build here\n",
    "trialPath = '/Users/sijinchen/Desktop/Cathy Caroline paper/test trial varying uncertainy/'\n",
    "\n",
    "paramDF = pd.read_csv('/Users/sijinchen/Desktop/Cathy Caroline paper/results files/SMEP model parameters.csv')\n",
    "phi = paramDF['SMEP_directed exploration_phi'].tolist()\n",
    "persev = paramDF['SMEP_perseveratiobonus'].tolist()\n",
    "ID = paramDF['ID'].tolist()\n",
    "session = paramDF['session_type'].tolist()\n",
    "\n",
    "directed_overall  = []\n",
    "random_overall = []\n",
    "exploit= []\n",
    "random_ore = []\n",
    "directed_ore = []\n",
    "exploit_abs= []\n",
    "random_abs = []\n",
    "directed_abs = []\n",
    "\n",
    "for num_id in range (1, 410):\n",
    "    df = pd.read_csv(datapath +  str(num_id) +'.csv')\n",
    "    print (num_id)\n",
    "\n",
    "    choice = df['choice'].tolist()\n",
    "    print (len(choice))\n",
    "    outcome = df['outcome'].tolist()\n",
    "    \n",
    "    trialDF = trial_by_trial_variables3(choice, outcome,phi[num_id-1], persev[num_id-1],ID[num_id-1],session[num_id-1])\n",
    "    print (len(trialDF))\n",
    "    trialDF.to_csv(trialPath +'//' + str(ID[num_id-1]) +'_' + str(session[num_id-1]) +  '_trial by trial measures4.csv')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = '/Users/sijinchen/Desktop/new data/new summary data_RL/'\n",
    "SMEPPath = '/Users/sijinchen/Desktop/new data/SMEP label new/'   ### new folder to build here\n",
    "\n",
    "paramDF = pd.read_csv('/Users/sijinchen/Desktop/new data/SMEP model parameters_new.csv')\n",
    "phi = paramDF['SMEP_directed exploration_phi'].tolist()\n",
    "persev = paramDF['SMEP_perseveratiobonus'].tolist()\n",
    "ID = paramDF['ID'].tolist()\n",
    "session = paramDF['session_type'].tolist()\n",
    "\n",
    "\n",
    "directed_overall  = []\n",
    "random_overall = []\n",
    "exploit= []\n",
    "random_ore = []\n",
    "directed_ore = []\n",
    "exploit_abs= []\n",
    "random_abs = []\n",
    "directed_abs = []\n",
    "\n",
    "for num_id in range (1, 74):\n",
    "    df = pd.read_csv(datapath + '//' + str(num_id) +'.csv')\n",
    "\n",
    "    choice = df['choice'].tolist()\n",
    "    outcome = df['outcome'].tolist()\n",
    "    \n",
    "\n",
    "    labelList = classify_trials(choice, outcome, phi[num_id-1], persev[num_id-1])\n",
    "    SMEPdict = {'SMEP label': labelList}\n",
    "    SMEPdf = pd.DataFrame(SMEPdict)\n",
    "    SMEPdf.to_csv(SMEPPath +'//' + ID[num_id-1] +'_' + session[num_id-1] +  '_SMEP label.csv')\n",
    "\n",
    "    num_exploit = labelList.count(1)\n",
    "    exploit_abs.append(num_exploit)\n",
    "    non_exploit = len(choice)-num_exploit\n",
    "    exploit.append(num_exploit/len(choice))\n",
    "\n",
    "    directed_overall.append(labelList.count(3)/len(choice))\n",
    "    random_overall.append(labelList.count(2)/len(choice))\n",
    "\n",
    "    directed_ore.append(labelList.count(3)/non_exploit)\n",
    "    random_ore.append(labelList.count(2)/non_exploit)\n",
    "\n",
    "    directed_abs.append(labelList.count(3))\n",
    "    random_abs.append(labelList.count(2))\n",
    "    \n",
    "\n",
    "\n",
    "resultDF = pd.DataFrame({'ID': ID, 'session': session, 'exploit trial percent': exploit, 'random exploration percent overall': random_overall, 'directed exploration percent overall': directed_overall, 'random exploration over all explore': random_ore, 'directed exploration over all explore': directed_ore, 'exploit trial num': exploit_abs, 'random exploration trial num': random_abs, 'directed exploration trial num': directed_abs})\n",
    "resultDF.to_csv('percent of SMEP trial type_new.csv')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## this version of classify_trials function takes into account perserveration trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_trials_with_persev(choices, rewards, phi, persev):\n",
    "    n_trials = len(rewards)\n",
    "    n_options = len(np.unique(choices))\n",
    "\n",
    "    # Initialize beliefs about mean rewards and uncertainties for each option\n",
    "    v = np.full(n_options, 0.5)  # Assuming binary rewards, 0.5 is a neutral initial belief\n",
    "    sig = np.full(n_options, np.sqrt(0.5 * (1 - 0.5)))  # Initial uncertainty based on Bernoulli variance\n",
    "    sig0 = 0.25\n",
    "\n",
    "    # label list\n",
    "    # exploit is 1, random  is 2, directed is 3, perserveration is 4\n",
    "    labelList = [2]  # first trial is random exploration\n",
    "\n",
    "    for t in range(1, n_trials):  # Start from 2nd trial as 1st choice is assumed random\n",
    "\n",
    "        chosen_option = choices[t]\n",
    "        previous_option = choices[t-1]\n",
    "\n",
    "        # Update beliefs based on previous choice and outcome\n",
    "        pe = rewards[t-1] - v[previous_option]\n",
    "        Kgain = sig[previous_option]**2 / (sig[previous_option]**2 + sig0**2)  # Assuming sigO=0.25 for binary rewards\n",
    "        v[previous_option] += Kgain * pe  # Update the mean belief\n",
    "        sig[previous_option] = np.sqrt((1 - Kgain) * sig[previous_option]**2)  # Update the uncertainty\n",
    "\n",
    "        eb = phi * sig  # Exploration bonus\n",
    "        utilities = v + eb\n",
    "\n",
    "        # Add perseveration bonus if applicable\n",
    "        if t > 1:\n",
    "            utilities[choices[t-1]] += persev\n",
    "\n",
    "        # Classify current trials\n",
    "        best_option = np.argmax(v)\n",
    "        most_uncertain_option = np.argmax(eb)\n",
    "        \n",
    "        if chosen_option == best_option:  ## exploitation trials\n",
    "            labelList.append(1)\n",
    "        elif chosen_option != best_option and chosen_option == previous_option:\n",
    "            labelList.append(4)\n",
    "        else:\n",
    "            # prev_best = 0   ## reset this to 0\n",
    "            if chosen_option == most_uncertain_option:  ## directed exploration\n",
    "                labelList.append (3)\n",
    "            else:                           ## random exploration\n",
    "                labelList.append (2)\n",
    "\n",
    "        \n",
    "\n",
    "    return labelList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = '/Users/sijinchen/Desktop/Cathy Caroline paper/summary data for RL/'\n",
    "SMEPPath = '/Users/sijinchen/Desktop/Cathy Caroline paper/SMEP label/'   ### new folder to build here\n",
    "\n",
    "paramDF = pd.read_csv('/Users/sijinchen/Desktop/Cathy Caroline paper/python code/SMEP model parameters.csv')\n",
    "phi = paramDF['directed exploration'].tolist()\n",
    "persev = paramDF['perseveration'].tolist()\n",
    "ID = paramDF['ID'].tolist()\n",
    "session = paramDF['session_type'].tolist()\n",
    "\n",
    "directed_overall  = []\n",
    "random_overall = []\n",
    "exploit_overall= []\n",
    "persev_overall = []\n",
    "\n",
    "random_ore = []\n",
    "directed_ore = []\n",
    "\n",
    "exploit_abs= []\n",
    "random_abs = []\n",
    "directed_abs = []\n",
    "persev_abs = []\n",
    "\n",
    "for num_id in range (1, 337):\n",
    "    df = pd.read_csv(datapath + '//' + str(num_id) +'.csv')\n",
    "\n",
    "    choice = df['choice'].tolist()\n",
    "    outcome = df['outcome'].tolist()\n",
    "\n",
    "    labelList = classify_trials_with_persev(choice, outcome, phi[num_id-1], persev[num_id-1])\n",
    "    SMEPdict = {'SMEP label': labelList}\n",
    "    SMEPdf = pd.DataFrame(SMEPdict)\n",
    "    SMEPdf.to_csv(SMEPPath +'//' + ID[num_id-1] +'_' + session[num_id-1] +  '_SMEP label.csv')\n",
    "\n",
    "    num_exploit = labelList.count(1)\n",
    "    num_persev = labelList.count(4)\n",
    "    exploit_abs.append(num_exploit)\n",
    "    non_exploit = len(choice)-num_exploit-num_persev\n",
    "\n",
    "    exploit_overall.append(num_exploit/len(choice))\n",
    "    directed_overall.append(labelList.count(3)/len(choice))\n",
    "    random_overall.append(labelList.count(2)/len(choice))\n",
    "    persev_overall.append(labelList.count(4)/len(choice))\n",
    "\n",
    "    directed_ore.append(labelList.count(3)/non_exploit)\n",
    "    random_ore.append(labelList.count(2)/non_exploit)\n",
    "\n",
    "    directed_abs.append(labelList.count(3))\n",
    "    random_abs.append(labelList.count(2))\n",
    "    persev_abs.append(labelList.count(4))\n",
    "    \n",
    "\n",
    "\n",
    "resultDF = pd.DataFrame({'ID': ID, 'session': session, 'exploit trial percent': exploit_overall, 'random exploration percent overall': random_overall, 'directed exploration percent overall': directed_overall, 'perserveration percent overall': persev_overall, 'random exploration over all explore': random_ore, 'directed exploration over all explore': directed_ore, 'exploit trial num': exploit_abs, 'random exploration trial num': random_abs, 'directed exploration trial num': directed_abs, 'perserveration trial num': persev_abs})\n",
    "resultDF.to_csv('percent of SMEP trial type_with perserv.csv')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## modify the code so that we are only looking at first 200 trials for BL2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SMEP(theta):\n",
    "    v1 = 0.5  # Prior belief mean reward value at trial 1\n",
    "    sig1 = 0.5  # Prior belief variance at trial 1\n",
    "    sigO = 0.25  # Observation variance\n",
    "    sigD = 0.1  # Diffusion variance\n",
    "    decay = 0.99  # Decay parameter\n",
    "    decay_center = 0.5  # Decay center\n",
    "\n",
    "    datapath = '/Users/sijinchen/Desktop/Cathy Caroline paper/summary data for RL/'\n",
    "    df = pd.read_csv(datapath + '//' + str(number) +'.csv')\n",
    "    user_id = df['user id'].tolist()[0]\n",
    "    session = df['session_type'].tolist()[0]\n",
    "\n",
    "    if session == 'BL2' or session == '6M' or session == '7M' or session == '12M':\n",
    "        choices= df['choice'].tolist()[:200]\n",
    "        rewards = df['outcome'].tolist()[:200]\n",
    "    else:\n",
    "        choices= df['choice'].tolist()\n",
    "        rewards = df['outcome'].tolist()\n",
    "        \n",
    "    beta = theta[0]  # Exploration bonus parameter\n",
    "    phi = theta[1]  # Uncertainty influence parameter\n",
    "    persev = theta[2]  # # Perseveration bonus parameter\n",
    "\n",
    "    # Initial values for reward belief and uncertainty\n",
    "    v = np.full(3, v1)  # Initial beliefs about mean rewards for each option\n",
    "    sig = np.full(3, sig1)  # Initial uncertainties for each option\n",
    "\n",
    "    # Store probability given model, use to calculate model likelihood\n",
    "    probList = []\n",
    "\n",
    "    for t in range(len(rewards)):\n",
    "\n",
    "        # Exploration and Perseveration Bonuses\n",
    "        eb = phi * sig  # Exploration bonus based on current uncertainty\n",
    "        pb = np.zeros(3)  # Perseveration bonus initialized to 0 for all options\n",
    "\n",
    "        if t == 0:  ### first trial\n",
    "             pChoice = 1/3   ### first choice is random, p (choice) is 0.33\n",
    "             probList.append(pChoice)\n",
    "\n",
    "        else:\n",
    "            pb[choices[t-1]] = persev  # Apply perseveration bonus to the previous choice, only tracking one trial back\n",
    "\n",
    "            # Update beliefs using a simplified Kalman filter approach\n",
    "            pe = rewards[t-1] - v[choices[t-1]]  # Prediction error\n",
    "            # print (pe)\n",
    "            Kgain = sig[choices[t-1]]**2 / (sig[choices[t-1]]**2 + sigO**2)  # Kalman gain\n",
    "            # print (Kgain)\n",
    "            v[choices[t-1]] += Kgain * pe  # Update the mean belief\n",
    "            # print (v)\n",
    "            sig[choices[t-1]] = np.sqrt((1 - Kgain) * sig[choices[t-1]]**2)  # Update the uncertainty\n",
    "\n",
    "            # Apply decay to beliefs and uncertainties\n",
    "            v = decay * v + (1 - decay) * decay_center\n",
    "            sig = np.sqrt(decay**2 * sig**2 + sigD**2)\n",
    "\n",
    "            # Action Selection based on the current Utility (value + bonuses)\n",
    "            utilities = v + eb + pb  # Calculate utilities for all options\n",
    "            # print (utilities)\n",
    "            probabilities = np.exp(utilities*beta) / np.sum(np.exp(utilities*beta))  # Softmax transformation to probabilities\n",
    "            # print (probabilities)\n",
    "            # print (choices[t])\n",
    "            pChoice = probabilities[choices[t]]\n",
    "            probList.append(pChoice)\n",
    "\n",
    "    ##log likelihood\n",
    "    logLike = (np.sum(np.log(probList)))*(-1)\n",
    "\n",
    "    return logLike\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_SMEP():\n",
    "\n",
    "    nll = []\n",
    "    betaList = []\n",
    "    phiList = []\n",
    "    persevList = []\n",
    "    IDlist = []\n",
    "\n",
    "\n",
    "    global number\n",
    "    for number in range (1,337):\n",
    "        IDlist.append (number)\n",
    "        print (\"*************************************************\")\n",
    "        print (\"This is ID number \" + str(number))\n",
    "\n",
    "\n",
    "        replication = 30\n",
    "        bnds = ((0.1,10),(0.01,10), (0,1),)\n",
    "\n",
    "        for h in range (replication):\n",
    "            # print (\"*************************************************\")\n",
    "            # print (\"This is replication number \" + str(h))\n",
    "            beta = np.random.uniform(0.5,2)  ## starting from moderate weight\n",
    "            phi = np.random.uniform(0.5,2)\n",
    "            persev = np.random.uniform(0,0.2) ## starting from low perseveration\n",
    "\n",
    "            startParams = [beta, phi, persev]\n",
    "            theta_optim = minimize(SMEP_with_L2, startParams, method = \"L-BFGS-B\", bounds = bnds, tol = 0.0005)\n",
    "\n",
    "            if h == 0:\n",
    "                minimizeLog = theta_optim.fun\n",
    "                optimX = theta_optim.x\n",
    "            else:\n",
    "                print (theta_optim.x)\n",
    "                print (theta_optim.fun)\n",
    "                if theta_optim.fun < minimizeLog:\n",
    "                    minimizeLog = theta_optim.fun\n",
    "                    optimX = theta_optim.x\n",
    "\n",
    "        nll.append(minimizeLog)\n",
    "        theta =  (optimX.tolist())\n",
    "        betaList.append(theta[0])\n",
    "        phiList.append(theta[1])\n",
    "        persevList.append(theta[2])\n",
    "     \n",
    "\n",
    "        dataDict = { 'numeric ID': IDlist,'random exploration': betaList, 'directed exploration': phiList, 'perseveration': persevList, 'model likelihood': nll}\n",
    "        data_df = pd.DataFrame(dataDict)\n",
    "        data_df.to_csv('SMEP model parameters_200 trials only.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_SMEP()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

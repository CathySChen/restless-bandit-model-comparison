{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import os\n",
    "import os.path\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import umap\n",
    "from sklearn.decomposition import PCA\n",
    "import math\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## first, run the functions needed to generate walks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''This function takes in a list of reward probabilities and a number that indicates the number of lags \n",
    "(usually set at lags = 50 but ot's flexible), and returns a list of correlation efficients at each lag. \n",
    "The length of this returned list should be the same number of lags'''\n",
    "def autocorr1(x,lags):\n",
    "    '''numpy.corrcoef, partial'''\n",
    "    corr = [1. if l==0 else np.corrcoef(x[l:],x[:-l])[0][1] for l in range (lags)]\n",
    "    return np.array(corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''This functions takes in a list of three lists of reward probabilities and returns a number that indicate the average half \n",
    "life of the walk.'''\n",
    "def halflife(walk):\n",
    "\n",
    "    armA = walk[0]\n",
    "    armB = walk[1]\n",
    "    armC = walk[2]\n",
    "    corr = []\n",
    "    halflife = 0\n",
    "\n",
    "    autoc1 = autocorr1(armA,50).tolist()\n",
    "    autoc2 = autocorr1(armB,50).tolist()\n",
    "    autoc3 = autocorr1(armC,50).tolist()\n",
    "\n",
    "    for m in range (len(autoc1)):\n",
    "        corr.append((autoc1[m]+autoc2[m]+autoc3[m])/3)\n",
    "\n",
    "    for p in range (len(corr)):\n",
    "        if corr[p]>= 0.5: \n",
    "            halflife += 1\n",
    "\n",
    "    return (halflife)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def walk():\n",
    "    while True:\n",
    "        ######################## SET PARAMETERS ########################\n",
    "        armNum = 3  ##3-armed bandit\n",
    "        walkList =[] ### contains three lists of rewawrd prob, one for each arm\n",
    "        initialProbList = [0.9,0.7,0.3]  ## set initial values \n",
    "        random.shuffle(initialProbList)  ## randomize inital values for each arm\n",
    "        hazard = 0.2  ## harzard rate\n",
    "        step = 0.2  ## step size\n",
    "        trialNum = 300  ## number of trials\n",
    "        bounds = [0.1,0.8] ## upper and lower bounds of reward probs\n",
    "        \n",
    "        for i in range (armNum):\n",
    "            ### first while loop aims to find an individual arm that walks up and down over 300 trials\n",
    "            #print ('individual')\n",
    "\n",
    "            #### step = 0.2, and hazard rate. = 0.33. Mean = 9.8 and sd < 2\n",
    "            while True:\n",
    "                armList = []\n",
    "                prob = initialProbList[i] ## add the first vaue in\n",
    "                armList.append(prob)\n",
    "\n",
    "                for j in range (trialNum-1):\n",
    "                    rand = np.random.random()\n",
    "                    if rand <= hazard: ### change occurs this trial, append the new prob\n",
    "                        if prob >= round (bounds[1],1):\n",
    "                            prob = prob - step\n",
    "                            armList.append(prob)\n",
    "\n",
    "                        elif prob <= round (bounds[0] , 1):\n",
    "                            prob = prob + step\n",
    "                            armList.append(prob)\n",
    "\n",
    "                        else:\n",
    "                            action = np.random.choice (['up', 'down'], 1)\n",
    "                            if action == 'up':\n",
    "                                prob = prob + step\n",
    "                                armList.append(prob)\n",
    "                            else:\n",
    "                                prob = prob - step\n",
    "                                armList.append(prob)\n",
    "                    else:  ## no change occurs on this trial, append the same prob\n",
    "                        armList.append(prob)\n",
    "\n",
    "\n",
    "                ### defining some criteria for individual walks\n",
    "\n",
    "                ### for each walk, the range (maximum-minimum) should be over 0.6\n",
    "                maxDiff = max(armList)-min(armList)\n",
    "\n",
    "                ### the average of each walk should be around 50% (48%-52%)\n",
    "                average = np.average(armList)\n",
    "                \n",
    "    \n",
    "\n",
    "                #myList = list(np.around(np.array(armList),2))*100\n",
    "\n",
    "                if maxDiff >= 0.6 and 0.48 <= average <=0.52 :\n",
    "                    break\n",
    "            \n",
    "            myList = list(np.around(np.array(armList),2))\n",
    "            walkList.append(myList) \n",
    "            \n",
    "        halfLife = halflife (walkList)\n",
    "        \n",
    "        ### the half life of autocorrelation of the alk should have mean = 10, sd = 2\n",
    "        if 8 <= halfLife <= 15:\n",
    "            break\n",
    "\n",
    "    richness = np.sum(walkList)/trialNum\n",
    "            \n",
    "    return (walkList, halfLife, richness)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "walkList = walk()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define a noisy win-stay lose-shift agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def WSLS_agent (walkList, bias):\n",
    "\n",
    "    ### initialize some parameters\n",
    "    choiceList = []\n",
    "    outcomeList = []\n",
    "    trialNum = 300\n",
    "\n",
    "    ### determine the first choice, which is random\n",
    "    firstChoice = np.random.choice(3)\n",
    "    choiceList.append(firstChoice)\n",
    "    currentChoice = firstChoice\n",
    "\n",
    "    ### determine whether the current choice is rewarded or not, based on the walk\n",
    "    for i in range (trialNum):\n",
    "        rewardProb = walkList[currentChoice][i]\n",
    "        rand = np.random.random()\n",
    "        if rand <= rewardProb:\n",
    "            reward = 1\n",
    "        else: \n",
    "            reward = 0\n",
    "        outcomeList.append(reward)\n",
    "    \n",
    "        ### determine the next choice based on the current choice and reward outcome\n",
    "        if i == trialNum-1:\n",
    "            continue\n",
    "        else:\n",
    "            if reward == 1:\n",
    "                pStay = 1-bias/2\n",
    "                rand2 = np.random.random()\n",
    "                if rand2 <= pStay:  ## next choice is repeat\n",
    "                    currentChoice = currentChoice\n",
    "                else:\n",
    "                    otherChoice = [0,1,2]\n",
    "                    otherChoice.pop(currentChoice)\n",
    "                    currentChoice = np.random.choice(otherChoice)\n",
    "            elif reward == 0:\n",
    "                pShift = 1-bias/2\n",
    "                rand2 = np.random.random()\n",
    "                if rand2 <= pShift:  ## next choice is shift\n",
    "                    otherChoice = [0,1,2]\n",
    "                    otherChoice.pop(currentChoice)\n",
    "                    currentChoice = np.random.choice(otherChoice)\n",
    "                else:\n",
    "                    currentChoice = currentChoice\n",
    "            choiceList.append (currentChoice)\n",
    "        \n",
    "    return choiceList, outcomeList\n",
    "    \n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define a RL agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RL_agent (walkList, alpha, beta):\n",
    "\n",
    "    ### initialize some parameters\n",
    "    choiceList = []\n",
    "    outcomeList = []\n",
    "    trialNum = 300\n",
    "    Qvalue = [0,0,0]\n",
    "\n",
    "    ### determine the first choice, which is random\n",
    "    firstChoice = np.random.choice(3)\n",
    "    choiceList.append(firstChoice)\n",
    "    currentChoice = firstChoice\n",
    "\n",
    "    ### determine whether the current choice is rewarded or not, based on the walk\n",
    "    ### update Q value of coresponding choice\n",
    "    for i in range (trialNum):\n",
    "        rewardProb = walkList[currentChoice][i]\n",
    "        rand = np.random.random()\n",
    "        if rand <= rewardProb:\n",
    "            reward = 1\n",
    "            Qvalue [currentChoice] += alpha*(reward-Qvalue[currentChoice])\n",
    "        else: \n",
    "            reward = 0\n",
    "            Qvalue [currentChoice] += alpha*(reward-Qvalue[currentChoice])\n",
    "        outcomeList.append(reward)\n",
    "\n",
    "        ## determine the next choice based on the current choice and reward outcome\n",
    "        prob0 = (np.exp((Qvalue[0])*beta))/(np.exp((Qvalue[0])*beta)+np.exp((Qvalue[1])*beta)+np.exp((Qvalue[2])*beta))\n",
    "        prob1 = (np.exp((Qvalue[1])*beta))/(np.exp((Qvalue[0])*beta)+np.exp((Qvalue[1])*beta)+np.exp((Qvalue[2])*beta))\n",
    "\n",
    "        rand1 = np.random.random()\n",
    "        if rand1 <= prob0:\n",
    "            currentChoice = 0\n",
    "        elif prob0 < rand1 <= (prob0+prob1):\n",
    "            currentChoice = 1\n",
    "        else:\n",
    "            currentChoice = 2\n",
    "        choiceList.append (currentChoice)\n",
    "\n",
    "    return choiceList, outcomeList\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define a foraging agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def foraging_agent (walkList, alpha, beta, rho):\n",
    "\n",
    "    ### initialize some parameters\n",
    "    choiceList = []\n",
    "    outcomeList = []\n",
    "    trialNum = 300\n",
    "    V_oit = 0.5\n",
    "\n",
    "    ### determine the first choice, which is random\n",
    "    firstChoice = np.random.choice(3)\n",
    "    currentChoice = firstChoice\n",
    "\n",
    "    ### determine whether the current choice is rewarded or not, based on the walk\n",
    "    ### update value of exploiting\n",
    "    for i in range (trialNum):\n",
    "        rewardProb = walkList[currentChoice][i]\n",
    "        rand = np.random.random()\n",
    "        if rand <= rewardProb:\n",
    "            reward = 1\n",
    "            V_oit += alpha*(reward-V_oit)\n",
    "        else: \n",
    "            reward = 0\n",
    "            V_oit += alpha*(reward-V_oit)\n",
    "        outcomeList.append(reward)\n",
    "\n",
    "        ## determine whether to exploit or to explore\n",
    "        prob_oit = 1/(1+(np.exp(-1*(V_oit-rho)*beta)))\n",
    "\n",
    "        rand1 = np.random.random()\n",
    "        if rand1 <= prob_oit:  ### exploit the same option\n",
    "            currentChoice = currentChoice\n",
    "        else: ### explore other options and reset v_oit back to 0.5\n",
    "            otherChoice = [0,1,2]\n",
    "            otherChoice.pop(currentChoice)\n",
    "            currentChoice = np.random.choice(otherChoice)\n",
    "            V_oit = 0.5\n",
    "            \n",
    "        choiceList.append (currentChoice)\n",
    "\n",
    "    return choiceList, outcomeList\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot (walkList, choice, outcome):\n",
    "    fig = plt.figure (figsize= (30,5))\n",
    "    ax = fig.add_subplot(111)   \n",
    "\n",
    "    color = ['orange','green','blue','pink','grey']\n",
    "    for h in range (len(walkList)):\n",
    "        plt.plot(walkList[h], color = color[h])\n",
    "\n",
    "    ax.set_ylabel ('reward probability')\n",
    "\n",
    "    trialList = np.linspace(0, 300, 301)\n",
    "    for i in range (len(choice)):\n",
    "        if choice[i] == 0:\n",
    "            plt.scatter(trialList[i], 1.2, color = 'orange', marker = 's')\n",
    "        if choice[i] == 1:\n",
    "            plt.scatter(trialList[i], 1.2, color = 'green', marker = 's')\n",
    "        if choice[i] == 2:\n",
    "            plt.scatter(trialList[i], 1.2, color = 'blue', marker = 's')\n",
    "        if outcome[i] == 0:\n",
    "            plt.scatter(trialList[i], 1.4, color = 'grey', marker = 's')\n",
    "        if outcome[i] == 1:\n",
    "            plt.scatter(trialList[i], 1.4, color = 'black', marker = 's')\n",
    "\n",
    "    plt.axis([1,300,0,1.5])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test if the foraging agent works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "walkList, halfLife, richness = walk()\n",
    "alpha = 0.8\n",
    "beta = 1\n",
    "rho = 0.1\n",
    "\n",
    "choice, outcome = foraging_agent (walkList, alpha, beta, rho)\n",
    "plot(walkList, choice, outcome)\n",
    "\n",
    "print (np.mean(walkList))\n",
    "print (outcome.count(1)/len(outcome))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define a main function that runs the simulation, noist WSLS agents with different level of bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replication = 500\n",
    "recodeList = []\n",
    "# relativeRecodeList = []\n",
    "groupList = []\n",
    "# biasLevel = [0]\n",
    "biasLevel = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\n",
    "halfLifeList = []\n",
    "richnessList = []\n",
    "pRewardList = []\n",
    "\n",
    "\n",
    "for i in range (replication):\n",
    "    walkList, halfLife, richness = walk()\n",
    "    for bias in (biasLevel): ### the number of noisy WSLS agent\n",
    "        choice, outcome = WSLS_agent (walkList, bias)\n",
    "        groupList.append(bias)\n",
    "        halfLifeList.append(halfLife)\n",
    "        richnessList.append(richness)\n",
    "        pRewardList.append(outcome.count(1)/len(outcome))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define a main function that runs the simulation, RL agents with different level of learning rate and noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replication = 200\n",
    "recodeList = []\n",
    "# relativeRecodeList = []\n",
    "alphaList = []\n",
    "betaList = []\n",
    "# biasLevel = [0]\n",
    "alphaLevel = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\n",
    "betaLevel = [0.1, 0.2, 0.3, 0.5, 0.6, 0.8, 1, 1.2, 1.5, 2]\n",
    "# betaLevel = [0.01, 0.05, 0.1, 0.25, 0.5, 0.8, 1, 1.5, 2.5, 5]\n",
    "alpha = 0.9\n",
    "# beta = 1.5\n",
    "halfLifeList = []\n",
    "richnessList = []\n",
    "pRewardList = []\n",
    "\n",
    "\n",
    "for i in range (replication):\n",
    "    walkList, halfLife, richness = walk()\n",
    "    for h in range (len(alphaLevel)):\n",
    "        for j in range (len(betaLevel)): ### the number of noisy WSLS agent\n",
    "            choice, outcome = RL_agent (walkList, alphaLevel[h], betaLevel[j])\n",
    "            alphaList.append(alphaLevel[h])\n",
    "            betaList.append (betaLevel[j])\n",
    "            halfLifeList.append(halfLife)\n",
    "            richnessList.append(richness)\n",
    "            pRewardList.append(outcome.count(1)/len(outcome))\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define a main function that runs RL foraging agents with different learning rate, noise, and threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replication = 200\n",
    "recodeList = []\n",
    "# relativeRecodeList = []\n",
    "alphaList = []\n",
    "betaList = []\n",
    "rhoList = []\n",
    "\n",
    "alphaLevel = [0.1, 0.2, 0.3, 0.4, 0.5, 0.7, 0.8, 0.9, 1]\n",
    "betaLevel = [0.1, 0.3, 0.4, 0.6, 0.8, 0.9, 1, 1.2, 2, 2.5, 3, 5,8]\n",
    "rhoLevel = [0.05, 0.1, 0.2, 0.25, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\n",
    "\n",
    "# betaLevel = [0.01, 0.05, 0.1, 0.25, 0.5, 0.8, 1, 1.5, 2.5, 5]\n",
    "# alpha = 0.9\n",
    "# beta = 1.5\n",
    "halfLifeList = []\n",
    "richnessList = []\n",
    "pRewardList = []\n",
    "\n",
    "\n",
    "for i in range (replication):\n",
    "    walkList, halfLife, richness = walk()\n",
    "    for k in range (len(rhoLevel)):\n",
    "        for h in range (len(alphaLevel)):\n",
    "            for j in range (len(betaLevel)): \n",
    "                choice, outcome = foraging_agent (walkList, alphaLevel[h], betaLevel[j], rhoLevel[k])\n",
    "                alphaList.append(alphaLevel[h])\n",
    "                betaList.append (betaLevel[j])\n",
    "                rhoList.append (rhoLevel[k])\n",
    "                halfLifeList.append(halfLife)\n",
    "                richnessList.append(richness)\n",
    "                pRewardList.append(outcome.count(1)/len(outcome))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
